{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 3: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:** In the context of Convolutional Neural Networks (CNNs), a receptive field refers to the region of the input data that a particular neuron is sensitive to. Each neuron in a CNN has a receptive field that defines the area in the input space from which it receives information.\n",
    "\n",
    "Receptive fields are typically small, localized regions in the input space, and they are defined by the size of the filters used in the convolutional layers of the CNN. When a filter is applied to the input data through convolution, it scans the input and produces a feature map by computing dot products between the filter weights and the corresponding input values. The receptive field of a neuron in a particular layer is determined by the spatial extent of the filters that are applied to the previous layer's feature map.\n",
    "\n",
    "Receptive fields are important because they capture local spatial information from the input data. Neurons with small receptive fields are sensitive to fine details and capture local patterns, while neurons with larger receptive fields capture more global patterns. The hierarchical arrangement of layers in a CNN allows for the progressive enlargement of receptive fields, enabling the network to learn complex features at different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Controlling the rate at which the receptive field grows in a Convolutional Neural Network (CNN) can be achieved through three common methods: stride, pooling, and dilated convolution.\n",
    "\n",
    "Stride involves adjusting the step size of the filter during convolution. Increasing the stride value reduces the spatial dimensions of the feature maps, leading to a larger receptive field. However, this approach comes at the cost of losing fine-grained spatial information, as certain regions of the input are skipped.\n",
    "\n",
    "Pooling layers downsample the feature maps by aggregating information from non-overlapping regions. This process increases the receptive field size by capturing the most prominent features within each region. However, pooling can result in a loss of spatial resolution and decreased localization accuracy.\n",
    "\n",
    "Dilated convolution, or atrous convolution, expands the receptive field without changing the filter size. By introducing gaps or dilation between the filter values, dilated convolution captures larger contextual information while preserving the spatial resolution of the input. This makes it particularly useful for tasks that require precise localization, such as image segmentation or object detection.\n",
    "\n",
    "The choice of method for controlling the receptive field growth depends on the specific task at hand. Stride and pooling are commonly used for downsampling and extracting coarse features, which are suitable for tasks like image classification where global context is important. On the other hand, dilated convolution is well-suited for tasks that demand precise localization while maintaining high-resolution feature maps. Each approach has its advantages and trade-offs, and selecting the appropriate method depends on the specific requirements and objectives of the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:48.951893Z",
     "start_time": "2023-07-22T10:02:47.623460Z"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:36.176235Z",
     "iopub.status.busy": "2023-07-22T11:06:36.175855Z",
     "iopub.status.idle": "2023-07-22T11:06:37.118241Z",
     "shell.execute_reply": "2023-07-22T11:06:37.117887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "To calculate the receptive field of a neuron, we employ the following closed-form formulas\n",
    "\n",
    "$$ {Receptive Field (RF)}_i = {{Receptive Field (RF)}}_{i-1} + j \\cdot ((k - 1) \\cdot d) \\quad \\text{{and}} \\quad j = j \\cdot s $$\n",
    "\n",
    "$ \\text{where:} $\n",
    "\\begin{align*}\n",
    "\\text{Receptive Field (RF)}_i & \\text{ is the receptive field at the current layer } i, \\\\\n",
    "\\text{Receptive Field (RF)}_{i-1} & \\text{ is the receptive field at the previous layer } (i-1), \\\\\n",
    "k & \\text{ is the layer kernel size,} \\\\\n",
    "d & \\text{ is the layer dilation,} \\\\\n",
    "j & \\text{ is the layer jump value (denotes how many pixels of the receptive fields of spatially neighboring features do not overlap in one direction), and} \\\\\n",
    "s & \\text{ is the layer stride.}\n",
    "\\end{align*}\n",
    "\n",
    "This formula is applied iteratively on each layer in the model, where the values are computed based on the values obtained for the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:16:41.343520Z",
     "start_time": "2023-07-22T10:16:41.336929Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.139568Z",
     "iopub.status.busy": "2023-07-22T11:06:37.139250Z",
     "iopub.status.idle": "2023-07-22T11:06:37.142561Z",
     "shell.execute_reply": "2023-07-22T11:06:37.142249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size (spatial extent) of the receptive field of each \"pixel\" in the output tensor is 112\n"
     ]
    }
   ],
   "source": [
    "## Answer:\n",
    "def calculate_receptive_field(model_layers):\n",
    "    \"\"\"Calculates the receptive field of a convolutional neural network.\n",
    "\n",
    "    Args:\n",
    "    model_layers: A list of tuples, where each tuple contains the kernel size, stride,\n",
    "      and dilation rate of a layer.\n",
    "\n",
    "    Returns:\n",
    "    The receptive field of the output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    current_receptive_field = 1\n",
    "    layer_jump = 1\n",
    "    for kernel_size, stride, dilation in model_layers:\n",
    "        previous_receptive_field = current_receptive_field\n",
    "        current_receptive_field = previous_receptive_field + layer_jump * ((kernel_size - 1) * dilation)\n",
    "        layer_jump *= stride\n",
    "    return current_receptive_field\n",
    "\n",
    "model_layers = [(3, 1, 1), (2, 2, 1), (5, 2, 1), (2, 2, 1), (7, 1, 2)]\n",
    "receptive_field = calculate_receptive_field(model_layers)\n",
    "print('The size (spatial extent) of the receptive field of each \"pixel\" in the output tensor is', receptive_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:29:17.260416Z",
     "start_time": "2023-07-14T13:29:15.125098Z"
    }
   },
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "The presence of the skip connection in the residual mapping, given by $\\vec{y}_l = f_l(\\vec{x}; \\vec{\\theta}_l) + \\vec{x}$, is the reason for the observed differences in the learned filters between the original network and the residual network.\n",
    "\n",
    "In the original network, the mapping is $\\vec{y}_l = f_l(\\vec{x}; \\vec{\\theta}_l)$, where $\\vec{x}$ represents the input and $\\vec{\\theta}_l$ represents the learnable parameters of the layer.\n",
    "\n",
    "However, in the residual network, the skip connection adds the original input $\\vec{x}$ to the transformed features obtained from the layer, resulting in $\\vec{y}_l = f_l(\\vec{x}; \\vec{\\theta}_l) + \\vec{x}$. This alters the information flow within the network and modifies the optimization process. The network can now focus on learning the residual information, represented by the difference between the transformed features and the original input.\n",
    "\n",
    "As a consequence, the network can adapt its filters differently to capture and optimize the residual mapping. This leads to the observation of completely different filter patterns being learned in the residual network compared to the original network. The network learns to assign different importance and weights to the original input and the transformed features while updating the filters, resulting in distinct filter patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:48.955935Z",
     "start_time": "2023-07-22T10:02:48.953117Z"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.144237Z",
     "iopub.status.busy": "2023-07-22T11:06:37.144145Z",
     "iopub.status.idle": "2023-07-22T11:06:37.147205Z",
     "shell.execute_reply": "2023-07-22T11:06:37.146910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:42:57.747492Z",
     "start_time": "2023-07-14T13:42:57.728414Z"
    }
   },
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "To replace the two consecutive dropout layers with a single dropout layer, we need to determine the equivalent dropout probability `q` that achieves the same effect. The dropout probability `q` can be calculated as the complement of the probability that both dropout layers retain a particular unit.\n",
    "\n",
    "Since dropout layers are applied consecutively, the probability that a unit is retained in the first dropout layer is `1 - p1`, and the probability that the unit is retained in the second dropout layer is `1 - p2`. To calculate the overall probability of retaining a unit after both dropout layers, we multiply these probabilities together.\n",
    "\n",
    "Therefore, the value of `q` can be expressed as:\n",
    "```python\n",
    "q = 1 - (1 - p1) * (1 - p2)\n",
    "```\n",
    "This expression calculates the combined probability of retaining a unit after replacing the two consecutive dropout layers with a single dropout layer with probability `q`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "False. While the commonly used practice is to place dropout after the activation function in a neural network, it is not a strict requirement. Dropout can be placed before or after the activation function, and both configurations have been explored in the literature.\n",
    "\n",
    "Placing dropout after the activation function allows it to act on the transformed features and encourages the network to learn more robust representations. It helps prevent overfitting by randomly dropping out units during training.\n",
    "\n",
    "On the other hand, placing dropout before the activation function can also be beneficial. It can act as a form of input noise and regularize the network by randomly zeroing out a fraction of the input values.\n",
    "\n",
    "The choice between placing dropout before or after the activation function depends on the specific architecture, task, and experimental setup. It is an aspect of network design that can be adjusted based on empirical observations and performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "To prove that the scaling of activations by $1/(1-p)$ after applying dropout with a drop probability of $p$ is required to maintain the value of each activation unchanged in expectation, we can analyze the expected value of the activations before and after dropout.\n",
    "\n",
    "Let's consider an activation value $x$ before dropout. During dropout, with probability $p$, the activation is set to zero, and with probability $1-p$, the activation is retained. Therefore, the expected value of the activation after dropout, denoted as $\\mathbb{E}[\\tilde{x}]$, can be calculated as:\n",
    "\n",
    "$$\\mathbb{E}[\\tilde{x}] = 0 \\cdot p + x \\cdot (1-p) = x \\cdot (1-p).$$\n",
    "\n",
    "Now, if we scale the activations by $1/(1-p)$ after dropout, the scaled activation $\\hat{x}$ can be expressed as:\n",
    "\n",
    "$$\\hat{x} = \\frac{\\mathbb{E}[\\tilde{x}] \\cdot 1}{1-p} = \\frac{x \\cdot (1-p)}{1-p} = x.$$\n",
    "\n",
    "We observe that the scaled activation $\\hat{x}$ is equal to the original activation $x$. Therefore, the scaling factor of $1/(1-p)$ is necessary to maintain the expected value of each activation unchanged after dropout.\n",
    "\n",
    "By applying the scaling, we ensure that the activations have the same expected value both before and after dropout, preserving the information contained in the activations and preventing any biases introduced by dropout from affecting the overall network behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:48:31.593392Z",
     "start_time": "2023-07-14T13:48:31.558976Z"
    }
   },
   "source": [
    "1. You're training an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "Using Mean Squared Error (MSE) loss instead of Binary Cross-Entropy (log-loss) is possible but not recommended due to the following reasons:\n",
    "\n",
    "1. **Non-Convexity and Global Minimum**: MSE loss is not a convex function when the decision boundary is not linear. Consequently, the optimization process using MSE loss is not guaranteed to converge to the global minimum. In contrast, log-loss, which is convex, ensures that the optimization process converges to the global minimum. The non-convex nature of MSE loss can lead to suboptimal solutions and hinder the model's ability to learn complex decision boundaries.\n",
    "\n",
    "2. **Penalization of Wrong Predictions**: MSE loss penalizes wrong predictions less compared to log-loss. The squared error term in MSE loss reduces the impact of misclassifications, potentially making the model less incentivized to make correct predictions. On the other hand, log-loss penalizes incorrect predictions more heavily, particularly when the predicted probability is far from the true label. This encourages the model to strive for accurate predictions and improves its discriminative ability.\n",
    "\n",
    "For example, let's consider the following scenario:\n",
    "- True label: 1\n",
    "- Predicted probability: 0.1\n",
    "\n",
    "Using MSE loss, the error is computed as:\n",
    "$$ MSE = (1-0.1)^2 = 0.81 $$\n",
    "\n",
    "Using log-loss (Binary Cross-Entropy), the error is computed as:\n",
    "$$ LogLoss = -(1 \\cdot log(0.1) + (1-1) \\cdot log(1-0.1)) = 2.3 $$\n",
    "\n",
    "As observed, the MSE loss is much smaller than the log-loss. This indicates that the model trained with MSE loss will be less incentivized to make correct predictions, as the penalty for misclassifications is comparatively lower.\n",
    "\n",
    "Therefore, it is recommended to use log-loss or other appropriate loss functions specifically designed for classification tasks. These loss functions better capture the objectives of binary classification problems, encourage accurate predictions, and offer better convergence properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:48.959814Z",
     "start_time": "2023-07-22T10:02:48.956825Z"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.149404Z",
     "iopub.status.busy": "2023-07-22T11:06:37.149287Z",
     "iopub.status.idle": "2023-07-22T11:06:37.151945Z",
     "shell.execute_reply": "2023-07-22T11:06:37.151683Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**:\n",
    "The most likely cause for the loss reaching a plateau after only a few iterations and the model no longer training is the use of the sigmoid activation function throughout the model architecture.\n",
    "\n",
    "The sigmoid activation function has a saturation property, which means that for very large or very small inputs, the gradient approaches zero. This can lead to the vanishing gradient problem, where the gradients become extremely small and hinder the training process. As a result, the model fails to update its parameters effectively and gets stuck in a plateau.\n",
    "\n",
    "In the given model, the sigmoid activation function is applied repeatedly in the hidden layers (*24 times) after the initial linear layer. This amplifies the saturation effect and accelerates the vanishing gradient problem.\n",
    "\n",
    "To address this issue, alternative activation functions can be used that do not suffer from the vanishing gradient problem, such as ReLU (Rectified Linear Unit) or its variants (e.g., Leaky ReLU, ELU). These activation functions allow for better gradient flow and alleviate the saturation problem. By replacing the sigmoid activation function with a more suitable choice, the model is more likely to continue training effectively and avoid reaching an early plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "No, replacing the `sigmoid` activations with `tanh` activations would not necessarily solve the problem of the loss reaching a plateau and the model no longer training. While `tanh` activation function can mitigate some of the saturation issues associated with `sigmoid`, it may not completely solve the problem.\n",
    "\n",
    "The `tanh` activation function is an improvement over `sigmoid` as it has a steeper gradient and is centered around zero, which helps alleviate the vanishing gradient problem to some extent. However, it can still suffer from saturation for large inputs, leading to diminishing gradients and slow convergence.\n",
    "\n",
    "In the given model, the repeated use of the `tanh` activation function in the hidden layers could still result in the vanishing gradient problem, especially with 24 consecutive `tanh` layers. The gradients can become extremely small, making it difficult for the model to update its parameters effectively and causing the loss to plateau.\n",
    "\n",
    "To address the issue of the loss plateauing and the model not training effectively, it is recommended to consider alternative activation functions that are less prone to saturation and better promote gradient flow. ReLU (Rectified Linear Unit) and its variants, such as Leaky ReLU or ELU, are commonly used in to mitigate the vanishing gradient problem and facilitate training. By replacing the `sigmoid` activations with a suitable choice like ReLU or its variants, it is more likely to overcome the plateau issue and improve the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "    1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "    2. The gradient of ReLU is linear with its input when the input is positive.\n",
    "    3. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. False. While ReLU activations help mitigate the vanishing gradient problem by preventing gradients from vanishing for positive inputs, they can still suffer from the dying ReLU problem. This occurs when the input to a ReLU unit becomes negative and remains negative, causing the gradient to be zero. In this case, the gradients will vanish, leading to dead neurons and hindering the training process.\n",
    "\n",
    "2. True. When the input to ReLU is positive, the gradient is a constant value of 1. The derivative of ReLU with respect to its input is 1 for positive input values, indicating a linear relationship between the input and the gradient. This linear characteristic simplifies the gradient computations and helps alleviate the vanishing gradient problem for positive inputs.\n",
    "\n",
    "3. True. ReLU activations can lead to dead neurons, also known as \"zero activation\" or \"dead ReLU\" neurons. If the input to a ReLU unit becomes negative and remains negative during training, the unit will output zero for all subsequent inputs. Dead neurons are problematic because they essentially become non-responsive and do not contribute to the learning process. Dead neurons can occur when the learning rate is set too high or when a large number of ReLU units receive negative inputs. Techniques such as leaky ReLU or parameter initialization can be used to mitigate the issue of dead neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**:\n",
    "Regular Gradient Descent (GD) updates the model's parameters by computing the average gradient over the entire dataset. It calculates the gradients for all training examples, performs a parameter update, and repeats this process iteratively. GD ensures convergence to the global minimum but can be computationally expensive for large datasets.\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates parameters after processing each individual training example. After shuffling the dataset, it randomly selects one batch at a time, calculates the gradient based on that example, and performs a parameter update. SGD is computationally efficient but introduces more noise and exhibits higher variance due to the random nature of individual examples. It may converge faster initially, but its convergence can be noisier compared to GD.\n",
    "\n",
    "Mini-Batch SGD combines the benefits of GD and SGD. It divides the dataset into small batches of fixed size and calculates the average gradient over each batch. The model's parameters are then updated based on these average gradients (i.e Mini-Batch iterates over all the dataset). Mini-Batch SGD strikes a balance between computational efficiency and stable updates. It reduces noise compared to SGD, leading to faster convergence, and provides a more stable optimization trajectory. Additionally, mini-batch SGD can take advantage of parallel computation by processing multiple batches concurrently.\n",
    "\n",
    "In summary, GD updates based on the average gradient over the entire dataset, SGD updates after processing each individual example, and Mini-Batch SGD updates based on average gradients computed over small batches. The choice depends on the specific requirements of the task, dataset size, and available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**:\n",
    "1. Two reasons why Stochastic Gradient Descent (SGD) is used more often in practice compared to Gradient Descent (GD) are:\n",
    "\n",
    "   a. Computational Efficiency: SGD is computationally more efficient than GD. In GD, the gradients for all training examples need to be calculated before a single parameter update, which can be time-consuming for large datasets. On the other hand, SGD updates parameters after processing each individual example, resulting in faster iterations and quicker convergence. This makes SGD more scalable and suitable for training on large datasets.\n",
    "\n",
    "   b. Generalization and Avoidance of Local Minima: SGD's inherent noise due to the random sampling of training examples helps prevent the algorithm from getting stuck in local minima. The noise introduced by SGD adds exploration to the optimization process, allowing the algorithm to escape shallow local optima and find better solutions. This property of SGD often leads to improved generalization and the ability to find better global optima compared to GD.\n",
    "\n",
    "2. Gradient Descent (GD) may not be suitable in the following cases:\n",
    "\n",
    "   a. Large Datasets: GD requires computing gradients for the entire dataset during each parameter update. When dealing with large datasets that cannot fit into memory, performing GD becomes challenging or even infeasible. The memory requirements and computational time can become extremely high.\n",
    "\n",
    "   b. Online Learning: GD assumes that the entire dataset is available upfront and requires multiple passes through the data for convergence. In scenarios where data arrives sequentially or in a streaming fashion, such as online learning, GD cannot be used directly. In such cases, SGD or other online learning algorithms that can adapt to new data points in real-time are more appropriate.\n",
    "\n",
    "   c. Non-Differentiable Loss Functions: GD relies on calculating gradients to update parameters, so it requires differentiable loss functions. If the loss function is non-differentiable, such as in cases involving discrete optimization or when using non-smooth loss functions, GD cannot be directly applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**:\n",
    "When increasing the mini-batch size from $B$ to $2B$, it is generally expected that the number of iterations required to converge to the same loss value $l_0$ will decrease.\n",
    "\n",
    "Increasing the batch size has several potential benefits:\n",
    "\n",
    "1. **More Accurate Gradient Estimation**: With a larger batch size, the gradient estimation becomes more accurate as it incorporates information from a larger subset of the training data. This improved gradient estimation can lead to faster convergence.\n",
    "\n",
    "2. **Improved Computational Efficiency**: Increasing the batch size can improve computational efficiency, especially on hardware with parallel processing capabilities like GPUs. Processing larger batches allows for better utilization of GPU resources and reduces the overhead associated with transferring data to and from GPU memory. This can lead to faster training iterations.\n",
    "\n",
    "However, it's important to note that there can be trade-offs when increasing the batch size. While larger batches can lead to faster convergence, they may also result in slower per-iteration computation due to the increased memory requirements and computation complexity. Additionally, larger batch sizes might also introduce challenges related to generalization and optimization dynamics.\n",
    "\n",
    "In summary, when increasing the mini-batch size from $B$ to $2B$, it is generally expected that the number of iterations required to converge to the same loss value $l_0$ will decrease. This is due to the more accurate gradient estimation and improved computational efficiency associated with larger batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "    1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "    2. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "    3. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "    4. Training with SGD requires more memory than with GD.\n",
    "    5. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "    6. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**\n",
    "1. False. When training a neural network with Stochastic Gradient Descent (SGD), we perform an optimization step for each subset, not for each sample in the dataset. In SGD (Stochastic Gradient Descent) the model's parameters are updated based on the gradients computed on randomly selected subsets of the training data.\n",
    "\n",
    "2. False. Gradients obtained with SGD have more variance compared to gradients obtained with GD. The stochastic nature of SGD, which uses randomly sampled mini-batches, introduces noise and fluctuations into the gradient estimates. This increased variance can lead to slower convergence compared to GD, where gradients are computed using the entire training dataset in each iteration. GD, with its more accurate gradient estimates, can converge more directly and smoothly towards the minimum.\n",
    "\n",
    "3. True. SGD is less likely to get stuck in local minima compared to GD. The inherent noise in SGD, resulting from the random sampling of mini-batches, introduces randomness and exploration to the optimization process. This noise helps the algorithm escape shallow local minima and move towards better solutions. GD, on the other hand, updates the parameters based on the average gradient computed over the entire dataset, which can make it more susceptible to getting stuck in local minima.\n",
    "\n",
    "4. False. Training with SGD typically requires less memory compared to GD. In SGD, only a mini-batch of the training data needs to be loaded into memory at a time, while in GD, the entire dataset is processed in each optimization step. Therefore, SGD requires less memory as it operates on smaller subsets of the data at a time.\n",
    "\n",
    "5. False. Neither SGD nor GD is guaranteed to converge to a global minimum in general. Both algorithms aim to minimize the loss function, but the convergence to a minimum depends on various factors such as the optimization landscape, network architecture, and the learning rate schedule. While SGD is more likely to escape local minima, it is not guaranteed to converge to a local minimum, and the same applies to GD and the global minimum. The convergence behavior depends on the specifics of the problem and the optimization process.\n",
    "\n",
    "6. True. Given a loss surface with a narrow ravine (high curvature in one direction), SGD with momentum is expected to converge more quickly than Newton's method, which doesn't have momentum. SGD with momentum helps in overcoming the problem of slow convergence in narrow ravines by accumulating momentum over time and effectively navigating along the ravine's steep and narrow path. Newton's method, although it considers the second-order information, may have difficulty navigating narrow ravines efficiently as it may get stuck due to high curvature. The momentum in SGD provides better exploration and facilitates quicker convergence in such scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Bonus** (we didn't discuss this at class):  We can use bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:\n",
    "True. In order to train a network that incorporates an embedded optimization problem as a layer using bi-level optimization, the inner optimization problem typically needs to be solved with a descent-based method such as Stochastic Gradient Descent (SGD), LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), or other optimization algorithms that operate by iteratively updating the parameters towards a minimum.\n",
    "\n",
    "Mathematically, bi-level optimization involves solving an optimization problem where one of the constraints or objectives itself depends on the solution of another optimization problem. In the context of deep learning, this can be represented as:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta_1} L(\\theta_1, \\theta_2) \\quad \\text{s.t.} \\quad \\theta_2 = \\arg \\min_{\\theta_2} \\varphi(\\theta_1, \\theta_2)\n",
    "$$\n",
    "\n",
    "Here, $\\theta_1$ represents the parameters of the outer-level optimization problem, and $\\theta_2$ represents the parameters of the inner-level optimization problem. The objective function $L(\\theta_1, \\theta_2)$ depends on the solution of the inner-level optimization problem, represented by $\\varphi(\\theta_1, \\theta_2)$.\n",
    "\n",
    "To train such a network, we need to update the parameters $\\theta_1$ through gradient-based optimization. This involves calculating gradients with respect to $\\theta_1$ and updating the parameters based on these gradients. The gradients of $L(\\theta_1, \\theta_2)$ with respect to $\\theta_1$ can be obtained through the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_1} L(\\theta_1, \\theta_2) = \\nabla_{\\theta_1} L(\\theta_1, \\arg \\min_{\\theta_2} \\varphi(\\theta_1, \\theta_2))\n",
    "$$\n",
    "\n",
    "To compute the gradients $\\nabla_{\\theta_1} L(\\theta_1, \\theta_2)$, we need to differentiate through the inner optimization process and obtain the gradients with respect to $\\theta_2$. This requires solving the inner optimization problem and obtaining the gradients with respect to $\\theta_2$.\n",
    "\n",
    "Descent-based methods like SGD, LBFGS, or other optimization algorithms are typically employed to solve the inner-level optimization problem and obtain the gradients with respect to $\\theta_2$. These methods iteratively update the parameters towards a minimum based on the gradient information. The obtained gradients with respect to $\\theta_2$ are then used to compute the gradients with respect to $\\theta_1$ and update the parameters of the outer-level optimization problem.\n",
    "\n",
    "Therefore, to train a network with an embedded optimization problem using bi-level optimization, it is necessary to solve the inner optimization problem with a descent-based method to obtain the gradients and enable the backpropagation of errors through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "    1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "     2. How can each of these problems be caused by increased depth?\n",
    "    3. Provide a numerical example demonstrating each.\n",
    "     4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:\n",
    "1. **Vanishing gradients** and **exploding gradients** are problems that can occur during the training of deep neural networks:\n",
    "\n",
    "   - Vanishing gradients: Vanishing gradients refer to the phenomenon where the gradients calculated during backpropagation become extremely small as they propagate from the output layer to the earlier layers of the network. When the gradients diminish significantly, it becomes challenging for the network to learn and update the parameters of the earlier layers effectively. This can result in slow convergence or even the inability of the network to learn meaningful representations.\n",
    "\n",
    "   - Exploding gradients: Exploding gradients occur when the gradients during backpropagation grow exponentially as they propagate backward through the layers of the network. This causes the gradients to become extremely large, leading to unstable parameter updates and difficulties in training the network. In extreme cases, the gradients can become so large that they result in numerical instability, such as overflow or NaN (not a number) values.\n",
    "\n",
    "2. Increased depth can contribute to both vanishing gradients and exploding gradients:\n",
    "\n",
    "   - Vanishing gradients with increased depth: As the depth of the network increases, the gradients have to pass through a larger number of layers during backpropagation. In deep networks, the gradients may experience multiple multiplicative operations through activation functions and weight matrices, such as in the chain rule of the gradient calculation. If the gradients are small initially or the weights are close to zero, the repeated multiplications can cause the gradients to diminish exponentially as they propagate backward through the layers, resulting in vanishing gradients.\n",
    "\n",
    "   - Exploding gradients with increased depth: Similarly, in deep networks, the gradients can also be amplified during the backpropagation process. If the weights in the network are initialized too large or there is a presence of large gradient updates, the repeated multiplications in each layer can lead to exponential growth of the gradients. This can cause the gradients to explode, leading to unstable training and difficulties in parameter updates.\n",
    "\n",
    "3. Numerical example demonstrating vanishing gradients and exploding gradients:\n",
    "\n",
    "   - Vanishing gradients example: Consider a deep network with multiple layers and sigmoid activation functions. Assume the weights are initialized close to zero. During backpropagation, the gradients calculated at each layer pass through the derivative of the sigmoid function, which has a maximum value of 0.25. As the gradients propagate through the layers, the repeated multiplications by 0.25 can cause the gradients to diminish significantly, resulting in vanishing gradients.\n",
    "\n",
    "   - Exploding gradients example: In a deep network with large weights or when there are large gradient updates, the gradients can become amplified during backpropagation. This amplification can lead to extremely large gradient values, potentially causing numerical instability or divergence during training.\n",
    "\n",
    "4. Determining whether the problem is vanishing gradients or exploding gradients without directly inspecting the gradient tensor(s) can be challenging. However, some indicators can help identify the issue. For example:\n",
    "\n",
    "   - Slow convergence or no improvement in training loss over time suggests vanishing gradients.\n",
    "   - Sudden instability, loss divergence, or NaN values in the loss or parameter updates can indicate exploding gradients.\n",
    "\n",
    "Additionally, monitoring the magnitudes of the gradients or weight updates during training can provide insights. If the gradients are consistently decreasing or approaching zero, it suggests vanishing gradients. Conversely, if the gradients are growing excessively or the weight updates become extremely large, it suggests exploding gradients. Analyzing the training dynamics, monitoring the loss curves, and inspecting the behavior of the training process can help infer whether vanishing or exploding gradients are the likely culprits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "**Answer**:\n",
    "To calculate the derivative of the final loss function $L_{\\mathcal{S}}$ with respect to each of the tensors $\\mat{W}_1$, $\\mat{W}_2$, $\\vec{b}_1$, $\\vec{b}_2$, and $\\vec{x}$, we can follow the chain rule and compute the partial derivatives for each term individually.\n",
    "\n",
    "1. Derivative w.r.t. $\\mat{W}_1$:\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\mat{W}_1} &= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mat{W}_1} \\right) + \\lambda \\mat{W}_1\n",
    "   \\end{align*}\n",
    "   where\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}} &= \\frac{-y^{(i)}}{\\hat{y}^{(i)}} + \\frac{1-y^{(i)}}{1-\\hat{y}^{(i)}}, \\\\\n",
    "   \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mat{W}_1} &= \\mat{W}_2 \\cdot \\varphi'(\\mat{W}_1 \\vec{x}^{(i)} + \\vec{b}_1) \\cdot \\vec{x}^{(i)}.\n",
    "   \\end{align*}\n",
    "\n",
    "2. Derivative w.r.t. $\\mat{W}_2$:\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\mat{W}_2} &= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mat{W}_2} \\right) + \\lambda \\mat{W}_2\n",
    "   \\end{align*}\n",
    "   where\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mat{W}_2} &= \\varphi(\\mat{W}_1 \\vec{x}^{(i)} + \\vec{b}_1).\n",
    "   \\end{align*}\n",
    "\n",
    "3. Derivative w.r.t. $\\vec{b}_1$:\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\vec{b}_1} &= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{b}_1} \\right)\n",
    "   \\end{align*}\n",
    "   where\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{b}_1} &= \\mat{W}_2 \\cdot \\varphi'(\\mat{W}_1 \\vec{x}^{(i)} + \\vec{b}_1).\n",
    "   \\end{align*}\n",
    "\n",
    "4. Derivative w.r.t. $\\vec{b}_2$:\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\vec{b}_2} &= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}}\n",
    "   \\end{align*}\n",
    "\n",
    "5. Derivative w.r.t. $\\vec{x}$:\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\vec{x}} &= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{\\partial \\ell(y^{(i)}, \\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{x}} \\right)\n",
    "   \\end{align*}\n",
    "   where\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{x}} &= \\mat{W}_2 \\cdot \\varphi'(\\mat{W}_1 \\vec{x}^{(i)} + \\vec{b}_1) \\cdot \\mat{W}_1.\n",
    "   \\end{align*}\n",
    "\n",
    "Note that $\\varphi'(\\cdot)$ represents the derivative of the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. The formula for the derivative can be used to compute gradients of neural network parameters numerically by approximating the derivative using finite differences. This involves perturbing the parameter values and computing the corresponding change in the loss function. The gradient is then estimated as the ratio of the change in the loss to the perturbation. This approach provides an alternative to automatic differentiation for computing gradients.\n",
    "\n",
    "2. Drawbacks of numerical gradient computation compared to automatic differentiation include computational inefficiency, numerical precision and stability issues, accumulation of discretization errors, and limited applicability to non-differentiable functions. Numerical gradient computation requires multiple function evaluations, making it computationally expensive for large networks. It is also sensitive to the choice of perturbation size and can be affected by numerical instability. Discretization errors can accumulate and affect the accuracy of the gradients. Furthermore, it cannot be used for functions that are not differentiable or have non-differentiable points. In contrast, automatic differentiation is efficient, accurate, handles numerical precision, avoids discretization errors, and can handle various types of functions, including non-differentiable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Given the following code snippet:\n",
    "    1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "     2. Calculate the same derivatives with autograd.\n",
    "    3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:49.003014Z",
     "start_time": "2023-07-22T10:02:48.961999Z"
    },
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.154845Z",
     "iopub.status.busy": "2023-07-22T11:06:37.154728Z",
     "iopub.status.idle": "2023-07-22T11:06:37.174857Z",
     "shell.execute_reply": "2023-07-22T11:06:37.174464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.4938, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "grad_W=tensor([[0.1057, 0.1057, 0.1057, 0.1057, 0.1057],\n",
      "        [0.1031, 0.1031, 0.1031, 0.1031, 0.1031],\n",
      "        [0.0901, 0.0901, 0.0901, 0.0901, 0.0901],\n",
      "        [0.0968, 0.0968, 0.0968, 0.0968, 0.0968],\n",
      "        [0.0897, 0.0897, 0.0897, 0.0897, 0.0897]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "grad_b=tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "Gradients for W: True\n",
      "Gradients for b: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "epsilon = 1e-6  # Small perturbation\n",
    "grad_W = torch.zeros_like(W)\n",
    "grad_b = torch.zeros_like(b)\n",
    "\n",
    "# Compute gradients numerically for W\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        perturbed_W = W.clone()\n",
    "        perturbed_W[i, j] += epsilon\n",
    "        perturbed_loss = foo(perturbed_W, b)\n",
    "        grad_W[i, j] = (perturbed_loss - loss) / epsilon\n",
    "\n",
    "# Compute gradients numerically for b\n",
    "for i in range(d):\n",
    "    perturbed_b = b.clone()\n",
    "    perturbed_b[i] += epsilon\n",
    "    perturbed_loss = foo(W, perturbed_b)\n",
    "    grad_b[i] = (perturbed_loss - loss) / epsilon\n",
    "\n",
    "print(f\"{grad_W=}\")\n",
    "print(f\"{grad_b=}\")\n",
    "\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "\n",
    "grad_W_numerical = grad_W\n",
    "grad_b_numerical = grad_b\n",
    "\n",
    "grad_W_close = torch.allclose(autograd_W, grad_W_numerical)\n",
    "grad_b_close = torch.allclose(autograd_b, grad_b_numerical)\n",
    "\n",
    "print(f\"Gradients for W: {grad_W_close}\")\n",
    "print(f\"Gradients for b: {grad_b_close}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "    1. Explain this term and why it's used in the context of a language model.\n",
    "     1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. **Word embeddings** are dense vector representations of words in a language model. Each word is mapped to a high-dimensional vector in such a way that semantically similar words are located closer to each other in the embedding space. These embeddings capture the relationships and meaning of words based on their contextual usage. They provide a way to represent words numerically, allowing language models to operate on continuous vector spaces rather than discrete tokens.\n",
    "\n",
    "   Word embeddings are used in language models to address the limitations of traditional one-hot encoding representations. One-hot encoding represents each word as a sparse binary vector, where only one element is 1 and the rest are 0. This representation lacks the ability to capture word similarities and relationships. In contrast, word embeddings provide a distributed representation where similar words have similar vector representations, enabling language models to leverage semantic information and generalize better to unseen words or sentences.\n",
    "\n",
    "2. It is possible but using embeddings will provide much better results. Training directly on sequences of tokens without embeddings would result in several consequences:\n",
    "\n",
    "   - **Lack of semantic understanding**: Without word embeddings, the model would treat each word as a separate entity with no inherent semantic relationship to other words. This would hinder the model's ability to capture contextual and semantic information crucial for understanding the meaning of sentences.\n",
    "\n",
    "   - **Limited generalization**: Word embeddings help models generalize to similar words and sentences that they haven't encountered during training. Without embeddings, the model would struggle to generalize to unseen words or sentences and may perform poorly on such inputs.\n",
    "\n",
    "   - **Increased dimensionality**: Training directly on sequences of tokens without embeddings would result in a high-dimensional input space, where each token is represented by a one-hot encoded vector. This can lead to a significant increase in the number of parameters, making the model computationally expensive and prone to overfitting.\n",
    "\n",
    "   Overall, word embeddings play a vital role in language models by providing a dense and continuous representation of words. They enable models to capture semantic relationships, generalize better, and reduce dimensionality, which is crucial for achieving better performance in tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. **Bonus**: How you would implement `nn.Embedding` yourself using only torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:49.079557Z",
     "start_time": "2023-07-22T10:02:49.004049Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.176602Z",
     "iopub.status.busy": "2023-07-22T11:06:37.176498Z",
     "iopub.status.idle": "2023-07-22T11:06:37.231621Z",
     "shell.execute_reply": "2023-07-22T11:06:37.231277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "The tensor `Y` contains the embeddings of the input tensor `X`. The shape of `Y` is `(5, 6, 7, 8, 42000)`.\n",
    "\n",
    "More specifically, each element in `X` represents an index value that selects a specific embedding vector from the embedding table. The embedding table is a tensor of shape `(num_embeddings, embedding_dim)` where `num_embeddings` is the total number of unique items to be embedded, and `embedding_dim` is the dimensionality of the embeddings.\n",
    "\n",
    "For each element in `X`, the corresponding embedding vector is extracted from the embedding table and placed in the corresponding position in `Y`. So, the value at each position in `Y` represents the embedding vector associated with the corresponding index value from `X`.\n",
    "\n",
    "In summary, `Y` contains the embedded representations of the elements in `X`, where each element in `X` is replaced by a high-dimensional embedding vector of size `embedding_dim`. The embedding vectors capture the relationships and meaning of the original elements in a continuous vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:02:49.150194Z",
     "start_time": "2023-07-22T10:02:49.081163Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-22T11:06:37.233389Z",
     "iopub.status.busy": "2023-07-22T11:06:37.233268Z",
     "iopub.status.idle": "2023-07-22T11:06:37.275037Z",
     "shell.execute_reply": "2023-07-22T11:06:37.274658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "# **Answer:**\n",
    "\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding_table = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor\n",
    "        embedded_x = self.embedding_table[x]  # Indexing to obtain embeddings\n",
    "        return embedded_x\n",
    "\n",
    "# Usage:\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = CustomEmbedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "    1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "    2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "    3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. True. Truncated Backpropagation Through Time (TBPTT) is a modification of the backpropagation algorithm specifically designed for recurrent neural networks (RNNs) with long sequences. The primary difference between TBPTT and standard backpropagation is that TBPTT breaks the long sequence into smaller subsequences of length S (truncated sequence length) during both forward and backward passes. This allows for efficient computation and prevents the vanishing or exploding gradients problem that can occur in RNNs.\n",
    "\n",
    "2. False. Limiting the length of the sequence provided to the model to length S is necessary for implementing TBPTT, but it is not the only requirement. In addition to the sequence length limitation, TBPTT also requires maintaining and updating hidden states across truncated subsequences during the forward pass, and backpropagating gradients through these subsequences during the backward pass. The hidden states need to be properly managed to ensure that the model captures long-term dependencies across the entire sequence.\n",
    "\n",
    "3. False. TBPTT does not explicitly limit the model's ability to learn relations between inputs that are at most S timesteps apart. The sequence length limitation in TBPTT is a computational constraint rather than a restriction on learning relationships. TBPTT breaks the long sequence into smaller subsequences to address the computational challenges associated with long sequences. However, the model can still learn and capture dependencies that span beyond the truncated sequence length S, as long as the information is propagated through the hidden states across different subsequences. The effectiveness of capturing long-term dependencies with TBPTT depends on the model architecture, sequence length, and the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 5 we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections, like in the tutorial..). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. The addition of the attention mechanism between the encoder and decoder introduces a form of alignment and information flow from the source sequence (encoder) to the target sequence (decoder) during the translation process. The hidden states that the encoder and decoder learn to generate become more contextually aware and adaptable.\n",
    "\n",
    "With attention, the hidden states in the encoder capture the relevant information in the source sequence while considering the importance of each source token for the target token being generated. The attention mechanism allows the encoder to assign different weights to different parts of the source sequence, focusing on the most relevant information for the current target token.\n",
    "\n",
    "Similarly, in the decoder, the attention mechanism enables the decoder to selectively attend to the relevant parts of the source sequence when generating each target token. The decoder's hidden states are influenced not only by the information encoded in the source sequence but also by the alignment weights determined by the attention mechanism.\n",
    "\n",
    "In comparison to a model without attention, the hidden states with attention are more adaptive, allowing the model to capture dependencies and align information across different positions in the source and target sequences. Attention helps address the limitation of fixed-length context vectors and allows the model to attend to different parts of the source sequence as needed during the translation process.\n",
    "\n",
    "2. If the model is modified to use self-attention, with the keys, queries, and values all equal to the encoder's hidden states, it would have an impact on the learned hidden states. Self-attention, also known as intra-attention, allows the model to capture dependencies within a single sequence or context.\n",
    "\n",
    "In this case, by using self-attention, the hidden states would have a stronger focus on the relationships and dependencies within the encoder's hidden states themselves. Each hidden state in the encoder would attend to the other hidden states, capturing the interactions and dependencies between different parts of the source sequence. This would enable the model to have a more fine-grained understanding of the dependencies within the source sequence and potentially capture long-range dependencies more effectively.\n",
    "\n",
    "By incorporating self-attention, the model would have a more comprehensive understanding of the context within the source sequence, which can facilitate better translation quality and capture more complex relationships within the sequence. This modification aligns with the trend in transformer models, which extensively utilize self-attention to capture dependencies across different positions in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. Without the KL-divergence term in the loss function, the images reconstructed by the model during training would likely appear similar to the original images but may lack certain desired properties. The reconstruction process of the VAE aims to reconstruct the input images as accurately as possible. However, without the KL-divergence term, the latent space may not be properly regularized, leading to less structured and less meaningful representations. The reconstructed images may lack diversity and exhibit lower quality compared to when the KL-divergence term is included.\n",
    "\n",
    "2. The effect on images generated by the model (from random samples in the latent space) would be more pronounced. Without the KL-divergence term, the latent space is not constrained to follow a specific distribution (usually a Gaussian distribution), resulting in uncontrolled and potentially less coherent generated images. The generated images may lack meaningful patterns and exhibit more random or noisy characteristics. The absence of the KL-divergence term can lead to a less structured and less diverse generation process, resulting in lower quality generated images compared to a VAE with the complete loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "     2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "     3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. False. The latent-space distribution generated by the model for a specific input image is not necessarily $\\mathcal{N}(\\vec{0},\\vec{I})$ (a multivariate Gaussian distribution with zero mean and identity covariance matrix). The latent space of a VAE is typically learned to follow a more complex distribution, often approximating a Gaussian distribution, but it does not have to strictly be of a specific form. The VAE's objective is to learn a latent space that captures the underlying structure and variability of the data, which may not always align with a simple Gaussian distribution.\n",
    "\n",
    "2. False. If we feed the same image to the encoder multiple times and decode each result, we are likely to get different reconstructions. The encoder maps the input image to a distribution in the latent space, which introduces stochasticity. Even for the same input image, the encoder may produce slightly different latent variables due to this stochasticity. Consequently, the decoded reconstructions from these different latent variables will also exhibit variations, resulting in different reconstructed images.\n",
    "\n",
    "3. True. The real VAE loss term, which includes the KL-divergence term, is indeed intractable to compute analytically. Therefore, during training, instead of directly optimizing this intractable term, we minimize an upper bound on the true loss. This upper bound is known as the Evidence Lower Bound (ELBO), and it consists of two components: the reconstruction loss (which measures the fidelity of the reconstructed output to the input) and the KL-divergence term (which encourages the learned latent space to stick to a prior distribution, typically a Gaussian distribution). By minimizing the ELBO, we aim to make this upper bound as tight as possible, hoping that it provides a good approximation of the true loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "    2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "    3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "    4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "    5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "1. True. In the context of GANs, the goal is to train the generator to generate realistic images that can fool the discriminator. Ideally, we want the generator's loss to be low, indicating that it can generate images that the discriminator has a hard time distinguishing from real images. Conversely, we want the discriminator's loss to be high, indicating that it can accurately differentiate between real and generated images. This adversarial objective encourages the generator to continuously improve and generate more realistic images.\n",
    "\n",
    "2. True. When training the discriminator, it is crucial to backpropagate the gradients into the generator. This is because the gradients computed by the discriminator provide valuable feedback to the generator on how to improve its generated images. By backpropagating into the generator, the gradients can update its parameters and guide it towards generating better images that can fool the discriminator. The interaction between the generator and discriminator through backpropagation is key to the adversarial training process of GANs.\n",
    "\n",
    "3. True. In many cases, to generate a new image with a GAN, we can sample a latent-space vector from a prior distribution, often a Gaussian distribution such as $\\mathcal{N}(\\vec{0},\\vec{I})$. This latent vector serves as input to the generator network, which transforms it into an image. By sampling from a prior distribution, we can explore different areas of the latent space to generate diverse and novel images.\n",
    "\n",
    "4. True. It can be beneficial to train the discriminator for a few epochs before training the generator. This allows the discriminator to reach a certain level of proficiency and develop a reasonable decision boundary between real and generated images. By giving the discriminator a head start, it can provide more meaningful feedback to the generator during training, leading to more effective learning. Training the discriminator first helps prevent the generator from initially producing arbitrary or poor-quality images that may confuse the discriminator and hinder training progress.\n",
    "\n",
    "5. False. If the generator is already generating plausible images and the discriminator has reached a stable state with 50% accuracy (random guessing), further training the generator may not necessarily lead to significant improvements in the generated images. At this point, the generator has already achieved a certain level of quality, and pushing it further may result in overfitting or diminishing returns. It is important to strike a balance in training the generator and discriminator and monitor the performance of the generated images to ensure continued improvement without overfitting or degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection and Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the diffrence between IoU and Dice score? what's the difference between IoU and mAP?\n",
    "    shortly explain when would you use what evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "The IoU measures the overlap between the predicted segmentation mask and the ground truth mask. It is calculated by dividing the area of intersection between the two masks by the area of their union. The IoU provides a measure of how well the predicted mask aligns with the ground truth mask, with a value of 1 indicating a perfect match.\n",
    "\n",
    "The Dice score, also known as the F1 score, is another metric used to evaluate the similarity between predicted and ground truth masks. It calculates the ratio of twice the area of their intersection to the sum of the areas of the predicted and ground truth masks. The Dice score is commonly used in medical image analysis and segmentation tasks.\n",
    "\n",
    "Mean Average Precision (mAP) is a metric primarily used in object detection tasks. It measures the performance of the object detection algorithm by considering the precision and recall trade-off across different object classes and varying detection thresholds. mAP provides an overall measure of the model's accuracy in detecting and localizing objects in images.\n",
    "\n",
    "When choosing an evaluation metric, consider the specific task and the desired evaluation criteria. IoU and Dice score are suitable for evaluating segmentation accuracy and overlap. On the other hand, mAP is more appropriate for evaluating the performance of object detection algorithms in detecting and localizing objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. regarding of YOLO and mask-r-CNN, which one is one stage detector? describe the RPN outputs and the YOLO output, address how the network produce the output and the shapes of each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer:**\n",
    "YOLO (You Only Look Once) is a one-stage object detection framework. It predicts bounding box coordinates, class probabilities, and confidence scores directly in a single pass. The output shape of YOLO depends on the grid size and the number of bounding boxes predicted per grid cell.\n",
    "\n",
    "Mask R-CNN, on the other hand, is a two-stage object detection framework. In the first stage, a Region Proposal Network (RPN) generates potential object proposals by predicting bounding box coordinates and objectness scores. The RPN output consists of anchor boxes and their corresponding objectness scores. In the second stage, the proposals are refined, and the network predicts accurate bounding boxes, class labels, and pixel-level segmentation masks.\n",
    "\n",
    "In summary, YOLO is a one-stage detector that directly predicts bounding boxes and class probabilities, while Mask R-CNN is a two-stage detector that uses an RPN for proposal generation and subsequent refinement stages for accurate detection and segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
